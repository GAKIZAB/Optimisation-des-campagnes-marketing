---
title: "Analyse de données avancées"
author: "Bertrand GAKIZA"
format: html
editor: visual
---


Dans ce projet, nous allons entreprendre une analyse approfondie d'un ensemble de données portant sur le comportement d'utilisation de cartes de crédit, englobant 8 636 titulaires de cartes actifs au cours des 6 derniers mois. Cet ensemble de données, qui comprend 17 variables comportementales à l'échelle du client, nous permettra de mieux comprendre les habitudes de consommation et les préférences de nos clients. En analysant ces données, nous visons à optimiser nos stratégies marketing en adoptant une approche de marketing ciblé.

Voici le dictionnaire de l'ensemble des données pour les cartes de crédit :


**SOLDE** : Montant du solde restant sur son compte pour effectuer des achats\
**BALANCE_FREQUENCY** : À quelle fréquence le solde est mis à jour, score entre 0 et 1 (1 = fréquemment mis à jour, 0 = pas fréquemment mis à jour)\
**PURCHASES** : Montant des achats effectués à partir du compte\
**ONEOFF_PURCHASES** : Montant maximum d’achat effectué en une seule fois\
**INSTALLMENTS_PURCHASES** : Montant de l’achat effectué en plusieurs fois\
**CASH_ADVANCE** : Paiement anticipé donné par l’utilisateur\
**PURCHASES_FREQUENCY** : À quelle fréquence les achats sont effectués, score entre 0 et 1 (1 = fréquemment acheté, 0 = pas fréquemment acheté\
**ONEOFFPURCHASESFREQUENCY** : À quelle fréquence les achats ont lieu en une seule fois (1 = acheté fréquemment, 0 = pas fréquemment acheté)\
**PURCHASESINSTALLMENTSFREQUENCY** : La fréquence à laquelle les achats en plusieurs fois sont effectués (1 = fréquemment effectué, 0 = pas fréquemment)\
**CASHADVANCEFREQUENCY** : La fréquence à laquelle l’argent est payé à l’avance \
**CASHADVANCETRX** : Nombre de transactions effectuées/
**PURCHASES_TRX « Cash in Advanced »**: Nombre de transactions d’achat effectuées\
**CREDIT_LIMIT** : Limite de la carte de crédit pour l’utilisateur\
**PAIEMENTS** : Montant du paiement effectué par l’utilisateur\
**MINIMUM_PAYMENTS** : Montant minimum des paiements effectués par l’utilisateur\
**PRCFULLPAYMENT** : Pourcentage du paiement intégral payé par l’utilisateur\
**DURÉE** : Durée du service de carte de crédit pour l’utilisateur

## Importation des données

```{r}
data <- read.csv("CC GENERAL.csv", row.names=1, sep=';')  
head(data)
```

```{r}
# Charger les librairies nécessaires
library(dplyr)
library(tidyr)
library(ggplot2)
library(FactoMineR)
library(factoextra)
library(cluster)
```

## Analyse descriptive
```{r}
summary(data)
```

### **Interpretation : **
Les données relatives aux cartes de crédit montrent que les soldes des clients sont diversifiés, avec une médiane de 1 601.2 et un maximum de 19 043,1, soulignant des comportements financiers hétérogènes. Les comportements d'achat présentent une médiane de 375.4 et un maximum de 49 399.57, illustrant des habitudes de consommation variées. Les avances en espèces sont peu sollicitées, avec une médiane de 0, mais peuvent atteindre 47 137.2 pour certains clients. Les limites de crédit varient également, avec une médiane de 3 000 et un maximum de 30 000, offrant à certains clients une flexibilité financière.
Ces données révèlent une clientèle aux caractéristiques financières diverses. Par conséquent, la classification est essentielle pour segmenter les clients en groupes homogènes et permettre un marketing ciblé.

### Méthodes

Nous disposons de données purement quantitatives, et plusieurs méthodes de classification peuvent être utilisées pour regrouper les observations en clusters homogènes. Ces méthodes se divisent en trois grandes approches : celles basées sur le calcul direct des distances, celles qui intègrent une réduction de dimension via l’Analyse en Composantes Principales (ACP), et le clustering non hiérarchique avec le k-means.

#### 1. Méthodes basées sur la distance : Distance + AGNES et Distance + HCLUST

**Distance + AGNES (Agglomerative Nesting)**  
AGNES repose sur le principe du Clustering Hiérarchique Ascendant, où chaque observation débute comme un cluster individuel. À chaque étape, l’algorithme fusionne les deux clusters les plus proches en minimisant la variance intra-cluster. Ce processus se poursuit jusqu'à obtenir un seul cluster englobant toutes les observations. Le résultat est un dendrogramme qui visualise les regroupements, permettant de choisir le nombre optimal de clusters.

**Distance + HCLUST**  
Cette méthode de clustering hiérarchique offre plus de flexibilité dans le choix des distances et des méthodes d’agrégation (comme Ward ou linkage complet). Elle fonctionne de manière similaire à AGNES, mais elle permet d’explorer diverses approches de regroupement. Le dendrogramme généré offre une visualisation hiérarchique des clusters, facilitant l'interprétation des relations entre les observations.

#### 2. Méthodes combinant réduction dimensionnelle et classification : ACP + HCPC

**Analyse en Composantes Principales (ACP)**  
L'ACP a pour objectif de réduire la dimensionnalité des données tout en préservant l’essentiel de l’information (variance). Elle transforme les variables quantitatives corrélées en composantes principales indépendantes, en retenant celles qui expliquent la plus grande part de la variance. Ce processus simplifie les données en évitant les problèmes de colinéarité, rendant l'analyse plus efficace.

**Classification Hiérarchique sur Composantes Principales (HCPC)**  
Le HCPC combine l’ACP avec un clustering hiérarchique, souvent basé sur la méthode de Ward. Après la réduction dimensionnelle, un clustering hiérarchique est appliqué dans l’espace des composantes principales. Les clusters obtenus peuvent être projetés sur les axes factoriels, facilitant ainsi leur interprétation et offrant une vue d’ensemble des regroupements.

#### 3. Classification non hiérarchique : k-means

**Principe et fonctionnement du k-means**  
Le k-means est une méthode de clustering non hiérarchique qui regroupe les observations en un nombre prédéfini de clusters. Il fonctionne en minimisant la variance intra-cluster. Les centres initiaux des clusters sont choisis aléatoirement, puis chaque observation est assignée au cluster dont le centre est le plus proche, selon la distance euclidienne. Les centres des clusters sont ensuite recalculés comme la moyenne des observations assignées. Ce processus d’assignation et de mise à jour se poursuit jusqu'à ce que les centres ne changent plus ou qu’un critère de convergence soit atteint.

**Détermination du nombre optimal de clusters**  
Le nombre optimal de clusters est déterminé par la méthode du coude, qui identifie un point où l’inertie intra-cluster diminue de manière significative. Des indices de validation, comme le coefficient silhouette, peuvent également être utilisés pour évaluer la qualité des clusters.








### **Comparaison des méthodes**

| Méthode               | Avantages                                                                                     | Limites                                                             |
|-----------------------|-----------------------------------------------------------------------------------------------|---------------------------------------------------------------------|
| **Distance + AGNES**  | - Visualisation claire via un dendrogramme                                                   | - Moins adapté pour de grands volumes de données                   |

| **Distance + HCLUST** | - Flexible avec différentes distances/méthodes                                               | - Nécessite de valider le nombre de clusters                       |

| **ACP + HCPC**        | - Gère les corrélations entre variables\ <br>- Clusters interprétables dans l’espace factoriel | - Peut perdre des informations si peu de composantes sont retenues |

| **k-means**           | - Rapide et efficace pour des grandes données<br>- Simple à implémenter                      | - Nécessite de fixer le nombre de clusters<br>- Sensible aux outliers |



### **Choix méthodologique**

Pour ce projet, nous avons choisi une approche combinant l'Analyse en Composantes Principales (ACP) avec le Clustering Hiérarchique par Partitionnement (HCPC) et la méthode k-means. L'ACP permet de réduire la dimensionnalité en conservant l'essentiel de l'information, tout en simplifiant l'analyse et en évitant la colinéarité. Le HCPC offre une visualisation intuitive des clusters à l'aide de dendrogrammes, rendant les groupes interprétables dans l'espace factoriel. Enfin, la méthode k-means est rapide et efficace pour regrouper les observations, particulièrement adaptée aux grandes bases de données quantitatives.


## **1. Méthode ACP + HCPC**

Pour cette analyse, nous avons commencé par la méthode ACP (Analyse en Composantes Principales) combinée avec la HCPC (Classification Hiérarchique des Partitions). Avant d'appliquer cette méthode, nous avons d'abord déterminé les variables actives et illustratives.

Variables actives :

"BALANCE"
"PURCHASES"
"CASH_ADVANCE"
"CREDIT_LIMIT"
"PAYMENTS"
"MIN_PAYMENTS"
Variables illustratives :

"BALANCE_FREQ"
"ONEOFF_PURCHASES"
"INSTALLMENTS_PURCHASES"
"PURCHASES_FREQ"
"ONEOFF_PURCHASES_FREQ"
"PURCHASES_INSTALLMENTS_FREQ"
"CASH_ADVANCE_FREQ"
"CASH_ADVANCE_TRX"
"PURCHASES_TRX"
"PRC_FULL_PAYMENT"
"TENURE"
Les variables illustratives approfondissent notre compréhension des comportements d'achat et des besoins financiers des clients. Elles décrivent notamment la fréquence des mises à jour de solde, les habitudes d'achat (ponctuelles ou en plusieurs versements), l'utilisation des cartes, les besoins en liquidité, ainsi que l'activité globale via le nombre total de transactions et le pourcentage de paiements complets.

## Diminution de dimension avec l'ACP

```{r}
# Calculer de l'ACP
res.acp <- PCA(data, ncp=2, scale.unit = T, quanti.sup = c(2,4,5,7,8,9,10,11,12,16,17))


```

```{r}
# Observer la decroissance de l'inertie 
res.acp$eig

```

```{r}
fviz_eig(res.acp, addlabels = T)
```


# Classification avec HCPC

```{r}
res.hcpc <-
  HCPC(
    res.acp ,
    nb.clust = -1,
    graph = TRUE
  )
```
Nous avons obtenu une classification en 5 classes distinctes, comme le révèle le dendrogramme issu de la méthode HCPC (Hierarchical Clustering on Principal Components), une technique permettant de regrouper des données en fonction de leurs similarités. Ces cinq classes représentent des groupes homogènes qui partagent des caractéristiques communes, ce qui peut aider à identifier des tendances ou des segments au sein des données analysées.


## Détermination de la qualité de la classification
## *1. Silhoutte*
Pour évaluer la qualité de notre classification, nous utilisons la **fonction silhouette**, une méthode qui mesure l'efficacité d'un clustering. Cette méthode évalue deux aspects essentiels :
1. **Cohésion** : À quel point un point est proche des autres points de son propre groupe.
2. **Séparation** : À quel point un point est éloigné des points des autres groupes.

La moyenne des scores de silhouette, qui varie entre -1 et 1, donne une indication globale de la qualité du clustering :
- **Moyenne proche de 1** : Clustering bien formé, les points sont clairement associés à leurs clusters.
- **Moyenne proche de 0** : Les clusters sont mal séparés ou se chevauchent.
- **Scores négatifs** : Indiquent des erreurs dans l'assignation des points, ceux-ci étant plus proches d’un autre cluster.


```{r}
# Convertir les clusters en un vecteur numérique
clusters <- as.numeric(as.factor(res.hcpc$data.clust$clust))

# Calcul de la matrice de distance
distance_matrix <- dist(res.acp$ind$coord)

# Calcul de l'indice de silhouette
silhouette_values <- silhouette(clusters, distance_matrix)

# Afficher les scores de silhouette
print("Coefficients de silhouette :")
summary(silhouette_values)

# Visualisation des silhouettes
fviz_silhouette(silhouette_values)

```
Avec une silhouette moyenne de 0,54, nous pouvons conclure que notre classification est modérément efficace. Cela indique que les clusters formés par notre HCPC présentent une séparation raisonnable.

## *2. Cophenetic*


```{r}
# Création du dendrogramme
hc <- hclust(dist(res.acp$ind$coord), method = "complete")
dend <- as.dendrogram(hc)

# Calcul du coefficient de cophenetic
coph <- cophenetic(hc)
coph_coef <- cor(coph, dist(res.acp$ind$coord))
cat("Coefficient de cophenetic :", coph_coef, "\n")
```
## Description des classes
```{r}
res.hcpc$desc.var
```
### **Interprétation enrichie et améliorée :**

#### **Cluster 1 : "Clients modérés et prudents"**
- **Caractéristiques principales :**
  - Fréquence et montants modérés dans toutes les catégories : **Cash Advance (0.088)**, **Achats en une fois (298)**, et **Achats totaux (589.6)**.
  - Solde bas (**705.2**) avec une limite de crédit plus faible que la moyenne (**3055.0**).
  - Fréquence des achats et paiements inférieure à la moyenne.
- **Interprétation :**
  - Ces clients utilisent leur carte avec parcimonie et sont probablement à faible risque de crédit. Ils privilégient les paiements minimums et ont une gestion financière conservatrice.

---

#### **Cluster 2 : "Utilisateurs axés sur les liquidités"**
- **Caractéristiques principales :**
  - Utilisation très élevée de **Cash Advance** : fréquence (**0.30**), montant (**2456.6**), et transactions (**7.71**).
  - Solde élevé (**3497.3**) et limite de crédit importante (**6346.7**).
  - Faible utilisation pour les achats en une fois ou échelonnés, fréquence d’achat modérée (**0.35**).
- **Interprétation :**
  - Ces clients exploitent principalement leur carte comme une source de liquidité rapide plutôt que pour des achats. Ils pourraient représenter un profil de risque à surveiller.

---

#### **Cluster 3 : "Acheteurs réguliers et actifs"**
- **Caractéristiques principales :**
  - Montants très élevés pour les **achats totaux (4459.3)**, **achats en une fois (2939.2)**, et **achats échelonnés (1521.0)**.
  - Fréquence d’achat élevée (**0.86**) et nombre de transactions bien au-dessus de la moyenne (**52.85**).
  - Limite de crédit élevée (**8821.4**) et paiements importants (**5131.3**).
- **Interprétation :**
  - Ces clients sont des utilisateurs intensifs qui maximisent les fonctionnalités de leur carte pour des achats réguliers et importants. Ils sont probablement des clients à forte valeur pour les programmes de fidélité.

---

#### **Cluster 4 : "Clients fortement dépendants des liquidités"**
- **Caractéristiques principales :**
  - Fréquence et montants exceptionnellement élevés de **Cash Advance (0.42)** et **transactions (14.17)**.
  - Solde très élevé (**7388.7**) et limite de crédit maximale (**11,027.7**).
  - Paiements élevés (**5902.7**), mais peu d’achats réguliers.
- **Interprétation :**
  - Ces clients utilisent leur carte presque exclusivement pour des avances de trésorerie importantes. Ils présentent potentiellement un profil de risque élevé et pourraient nécessiter une gestion proactive.

---

#### **Cluster 5 : "Clients haut de gamme et dépensiers"**
- **Caractéristiques principales :**
  - Montants exceptionnels pour les **achats totaux (25,503.4)**, **achats en une fois (19,694.5)**, et **achats échelonnés (5808.9)**.
  - Fréquence d’achat élevée (**0.90**) avec un nombre de transactions très élevé (**117.89**).
  - Limite de crédit maximale (**16,462.9**) et paiements conséquents (**27,548.3**).
- **Interprétation :**
  - Ces clients sont les plus dépensiers et exploitent pleinement les avantages de leur carte. Ils représentent un segment clé pour les offres haut de gamme et les programmes exclusifs.

---

### **Conclusion Générale :**
Les résultats montrent cinq segments bien différenciés :
1. **Cluster 1** : Utilisateurs modérés et prudents, faible activité, faible risque.
2. **Cluster 2** : Axés sur les liquidités, mais à surveiller pour leur dépendance à Cash Advance.
3. **Cluster 3** : Acheteurs actifs et réguliers, à forte valeur pour les campagnes de fidélité.
4. **Cluster 4** : Dépendants des avances de trésorerie, à fort potentiel de risque.
5. **Cluster 5** : Clients haut de gamme et dépensiers, cible idéale pour des offres premium.


```{r}
res.hcpc$desc.ind
```

# K-means**

```{r}

# Extraire les coordonnées des individus dans l'espace ACP
coordinates <- res.acp$ind$coord
# Déterminer le nombre de clusters à utiliser pour k-means
# Ici on va utiliser le nombre de clusters obtenu avec HCPC
nb.clust <- 5
# Effectuer le k-means
set.seed(123) # Pour la reproductibilité
kmeans_res <- kmeans(coordinates, centers = nb.clust)
# Ajouter les clusters k-means aux données
data$kmeans_cluster <- as.factor(kmeans_res$cluster)
# Visualiser les résultats du k-means
fviz_cluster(kmeans_res, data = coordinates, geom = "point",
ellipse.type = "convex", main = "K-means Clustering",
xlab = "Dimension 1", ylab = "Dimension 2") +
theme_minimal()

```


```{r}
# Calcul du coefficient de silhouette
silhouette_kmeans <- silhouette(kmeans_res$cluster, distance_matrix)

# Afficher les scores de silhouette
print("Coefficients de silhouette :")
summary(silhouette_kmeans)

# Visualisation des silhouettes
fviz_silhouette(silhouette_kmeans)

```

## Comparaison de HCPC et K-means

```{r}
# Extraire les groupes de la CAH
cah_clusters <- res.hcpc$data.clust$clust # Extraire les clusters de HCPC
# S'assurer que kmeans_res$cluster est un vecteur et non une liste
kmeans_clusters <- as.vector(kmeans_res$cluster)
# Créer une table de correspondance entre les clusters de la CAH et ceux du k-means
table(cah_clusters, kmeans_clusters)

```

Interpretation : L'analyse des clusters obtenus par les méthodes de classification hiérarchique (CAH) et de K-means met en lumière une correspondance générale satisfaisante, bien que certaines divergences soient notables. En effet, le premier cluster de la CAH se retrouve principalement en accord avec le premier cluster de K-means, tandis que le deuxième cluster de la CAH présente une correspondance marquée avec le troisième cluster de K-means, révélant ainsi une différence significative. En revanche, les clusters 4 et 5 de la CAH correspondent presque parfaitement à leurs équivalents. Bien que les deux approches génèrent des classifications globalement harmonieuses, des disparités notables se manifestent particulièrement dans les clusters 2 et 3.

### **3. Utilisation des Résultats:**

En se référant à la description des classes de classification hiérarchique, nous pouvons développer des solutions de marketing ciblé:
- Classes 1 & 4 : Produits de crédit et outils éducatifs visant à réduire la dépendance aux avances de trésorerie.
- Classe 2 : Récompenses modérées et incitations pour encourager une utilisation accrue.
- Classes 3 & 5 : Offres premium et programmes exclusifs destinés à renforcer la fidélité des clients à forte valeur.






The `echo: false` option disables the printing of code (only output is displayed).
